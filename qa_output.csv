Question,Answer
What is the arXiv ID of the text?, arXiv:2501.09223v1
What is the subject classification of this text?, cs.CL (Computational Linguistics)
When was the text submitted?," January 16, 2025"
Who are the authors of this text?, Tong Xiao and Jingbo Zhu
What institution are the authors affiliated with?, Northeastern University & NiuTrans Research
Under what license is this text distributed?, Creative Commons Attribution-NonCommercial 4.0 Unported License
What is the main topic of this text?, The foundations of Large Language Models (LLMs).
What is a key insight brought by LLMs?, Knowledge of the world and languages can be acquired through large-scale language modeling tasks.
How has the research methodology in NLP changed due to LLMs?, A shift from training specialized systems from scratch to using large-scale pre-training to obtain foundation models.
What is the purpose of this book?," To outline the basic concepts of LLMs and introduce related techniques, focusing on foundational aspects."
How many chapters are in the book?, Four
What is covered in Chapter 1?," The basics of pre-training, including methods and model architectures."
What is covered in Chapter 2?," Generative models (LLMs), scaling up model training, and handling long texts."
What is covered in Chapter 3?," Prompting methods for LLMs, including advanced methods like chain-of-thought reasoning."
What is covered in Chapter 4?," Alignment methods for LLMs, focusing on instruction fine-tuning and alignment based on human feedback."
What background knowledge is helpful for reading this book?," Machine learning, natural language processing, and an understanding of Transformers."
Is prior knowledge absolutely necessary to understand the book?," No, each chapter is designed to be self-contained."
What is the writing style of the book described as?, A compilation of notes taken while learning about LLMs.
What are the two types of problems typically involved in discussing pre-training in NLP?, Sequence modeling (encoding) and sequence generation.
What is the general equation used to represent a sequence model?," o = g(x0, x1, ..., xm; θ) = gθ(x0, x1, ..., xm)"
What are the two fundamental issues in pre-training NLP models?, Optimizing θ on a pre-training task and applying the pre-trained model to downstream tasks.
What are the three pre-training approaches discussed?," Unsupervised, supervised, and self-supervised pre-training."
What is unsupervised pre-training?, Optimizing a neural network using a criterion not directly related to specific tasks.
What is supervised pre-training?, Pre-training a neural network on supervised learning tasks before adapting it to downstream tasks.
What is self-supervised pre-training?, Training a neural network using supervision signals generated by itself from unlabeled data.
Which pre-training approach is most popular in current state-of-the-art NLP models?, Self-supervised pre-training.
What are the two major types of models used in NLP pre-training?, Sequence encoding models and sequence generation models.
What is sequence encoding?, Representing an input sequence as a vector or sequence of vectors.
What is sequence generation?, Generating a sequence of tokens based on a given context.
What is a common method of adapting pre-trained models?, Fine-tuning.
What is prompting?, Adapting LLMs to solve NLP problems by transforming them into text generation problems.
What is zero-shot learning?, Performing tasks not observed during the training phase.
What is few-shot learning?, Performing tasks with a few examples provided as demonstrations.
What are decoder-only architectures?," Architectures used in language models, predicting the distribution of tokens at a position given preceding tokens."
What is a common loss function used in decoder-only pre-training?, Log-scale cross-entropy loss.
What is masked language modeling?, Masking some tokens in the input sequence and training the model to predict them.
What is a drawback of masked language modeling?, The use of a special token ([MASK]) only during training.
What is permuted language modeling?, Predicting tokens in a permuted order to leverage broader context.
What is next sentence prediction (NSP)?, A task used in BERT to determine the relationship between two sentences.
What is the ELECTRA model?, A model that trains a Transformer encoder to identify whether tokens are original or altered.
What is the T5 model?, A model that frames various NLP tasks as text-to-text tasks.
What is preﬁx language modeling?, Predicting a subsequent sequence given a prefix sequence as context.
What is translation language modeling?, Pre-training on bilingual data to capture correspondences between tokens in two languages.
What is code-switching?, Switching among languages in a text.
What is catastrophic forgetting?, A neural network forgetting previously learned information when updated on new samples.
What are LLMs?," Large Language Models, often generative models capable of understanding and generating natural languages."
What is the goal of language modeling?, To predict the probability of a sequence of tokens occurring.
What is an autoregressive process in language modeling?, Predicting the next token based on preceding tokens.
What is the architecture of most LLMs based on?, Decoder-only Transformer architectures.
What are the two sub-layers in a Transformer block?, Self-attention and FFN (feed-forward network).
What is the purpose of the masking variable in self-attention?, To prevent the model from accessing the future (right-context) tokens.
What is the objective of maximum likelihood training for LLMs?, To maximize the likelihood of the training data.
What challenges arise when training LLMs at scale?," Requires large-scale distributed systems, significant computational resources, and potential training instability."
What is instruction fine-tuning?, Fine-tuning LLMs on instruction-response annotated data.
What is zero-shot learning in the context of LLMs?, The ability of fine-tuned LLMs to handle tasks not explicitly trained on.
What is reinforcement learning from human feedback (RLHF)?, A method that aligns LLMs with human values by using human feedback as a reward signal.
What are the two components in RLHF?, An agent (LLM) and a reward model.
What is a reward model?, A model that scores LLM outputs based on human preferences.
What is the role of prompting in LLM alignment?, To guide LLM behavior and elicit desired outputs.
What is in-context learning?, Learning from demonstrations or examples provided in the prompt.
What are three types of in-context learning?," Zero-shot, one-shot, and few-shot learning."
What is prompt engineering?, The process of designing effective prompts to maximize LLM performance.
What are some strategies for effective prompt engineering?," Clear task descriptions, guiding LLMs to think, providing reference information, and paying attention to format."
What is chain-of-thought (CoT) prompting?, Guiding LLMs to generate step-by-step reasoning for complex problems.
What are the differences between few-shot and zero-shot CoT prompting?," Few-shot CoT uses examples, while zero-shot CoT uses instructions to elicit step-by-step reasoning."
What is problem decomposition?," Breaking down a complex problem into smaller, simpler sub-problems."
What is least-to-most prompting?," A problem decomposition method that solves sub-problems sequentially, from least to most complex."
What is self-reﬁnement?, LLMs iteratively refining their own outputs based on feedback.
What are the three steps in a general self-reﬁnement framework?," Prediction, feedback collection, and refinement."
What is model ensembling?, Combining predictions from multiple LLMs or prompts.
What is the self-consistency method?, Selecting the answer that appears most frequently across multiple reasoning paths.
What is retrieval-augmented generation (RAG)?, Using external information retrieval systems to augment LLM inputs.
What is tool use in LLMs?," Integrating external tools (databases, APIs) into LLMs for improved accuracy and capabilities."
What is automatic prompt design?, Automatically creating and optimizing prompts for LLMs.
What are the components of a prompt optimization framework?," Prompt search space, performance estimation, and search strategy."
What are soft prompts?," Hidden, distributed representations of prompts embedded within LLMs."
What is context distillation?, Training a student model to use simplified instructions from a teacher model.
What is preﬁx fine-tuning?, Appending trainable vectors (preﬁxes) to the input of each Transformer layer.
What is prompt tuning?, Modifying only the embedding layer by adding trainable vectors (soft prompts).
How can soft prompts be used for parameter-efﬁcient ﬁne-tuning?," By only training a small number of parameters (the soft prompts), while keeping the main model parameters frozen."
How can soft prompts be used for context compression?, By distilling knowledge from long contexts into compact soft prompt representations.
What is prompt length reduction?, Simplifying prompts to reduce computational costs and improve efﬁciency.
What is the main goal of LLM alignment?," To ensure that LLM outputs align with human expectations, values, and preferences."
What are the three main approaches to LLM alignment?," Fine-tuning with labeled data, fine-tuning with reward models, and inference-time alignment."
What is supervised fine-tuning (SFT)?, Fine-tuning LLMs on a dataset of instruction-output pairs.
What are some challenges in manual generation of instruction-following data?," Requires significant effort, can be subjective, and may not cover all possible tasks."
What is self-instruct?, A method to automatically generate instruction-following data using LLMs.
What is the goal of instruction generalization?, Training LLMs to perform various tasks accurately as described by different instructions.
What is the superﬁcial alignment hypothesis?," The idea that core learning happens during pre-training, and fine-tuning primarily activates existing knowledge."
What is the weak-to-strong generalization problem?," Using smaller, less complex models to improve the training of larger, more complex models."
How can weak models be used to improve strong models in instruction fine-tuning?, By generating synthetic fine-tuning data or by incorporating knowledge distillation loss.
What is the performance gap recovered (PGR) metric?, A metric measuring the improvement in test-set performance achieved through weak-to-strong fine-tuning.
What is direct preference optimization (DPO)?, A method that directly optimizes the policy based on user preferences without training a reward model.
What is the main advantage of DPO?, Simplicity and sample efficiency compared to RLHF.
What is automatic preference data generation?, Using LLMs to generate preference labels for pairs of outputs.
What are some methods for automatic preference data generation?," Pairwise comparison, rating, and listwise ranking."
What is the overoptimization problem?, Decreasing performance due to over-aligning LLMs with an imperfect reward model.
How can the overoptimization problem be mitigated?, By combining multiple reward models.
What are ﬁne-grained rewards?, Rewards computed at a segment level to provide more detailed supervision.
What is Best-of-N (BoN) sampling?, Using a reward model to select the best output from multiple LLM predictions.
